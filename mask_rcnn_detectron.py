# -*- coding: utf-8 -*-
"""Copy of Mask_RCNN_Detectron.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kdkJgBStj0Cn5FNH5LZ0DPP-0G8ATew8
"""

import locale
locale.setlocale(locale.LC_ALL, 'en_US.UTF-8')

from google.colab import drive
drive.mount('/content/drive')

!nvidia-smi

!python -m pip install -q 'git+https://github.com/facebookresearch/detectron2.git'

# Now is a good time to confirm that we have the right versions of the libraries at our disposal.

import torch, detectron2
!nvcc --version
TORCH_VERSION = ".".join(torch.__version__.split(".")[:2])
CUDA_VERSION = torch.__version__.split("+")[-1]
print("torch: ", TORCH_VERSION, "; cuda: ", CUDA_VERSION)
print("detectron2:", detectron2.__version__)

# COMMON LIBRARIES
import os
import cv2

from datetime import datetime
from google.colab.patches import cv2_imshow

# DATA SET PREPARATION AND LOADING
from detectron2.data.datasets import register_coco_instances
from detectron2.data import DatasetCatalog, MetadataCatalog

# VISUALIZATION
from detectron2.utils.visualizer import Visualizer
from detectron2.utils.visualizer import ColorMode

# CONFIGURATION
from detectron2 import model_zoo
from detectron2.config import get_cfg

# EVALUATION
from detectron2.engine import DefaultPredictor

# TRAINING
from detectron2.engine import DefaultTrainer


from detectron2.data import build_detection_test_loader
from detectron2.evaluation import inference_on_dataset
from detectron2.evaluation import COCOEvaluator

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix
import seaborn as sns

base_folder = "/content/drive/MyDrive/AMPBA/Capstone/Model"

os.chdir(base_folder +"/train")

TRAIN_DATA_SET_NAME = "Train_Final"
TEST_DATA_SET_NAME = "Valid_Final"


register_coco_instances(TRAIN_DATA_SET_NAME, {}, base_folder + "/train/_annotations.coco.json", base_folder + "/train")
register_coco_instances(TEST_DATA_SET_NAME, {}, base_folder + "/valid/_annotations.coco.json", base_folder + "/valid")

# We can now confirm that our custom dataset was correctly registered using MetadataCatalog.

[
    data_set
    for data_set
    in MetadataCatalog.list()
    if data_set.endswith("Final")
]

import json
def load_annotations(file_path):
    with open(file_path, 'r') as f:
        data = json.load(f)
    category_ids = {cat['id']: idx + 1 for idx, cat in enumerate(data['categories'])}  # +1 to reserve 0 for background
    return category_ids

NUM_CLASSES = len(load_annotations('_annotations.coco.json'))
NUM_CLASSES

# HYPERPARAMETERS



# mask_rcnn_R_101_FPN_3x.yaml
# mask_rcnn_X_101_32x8d_FPN_3x

ARCHITECTURE = "mask_rcnn_R_101_FPN_3x"
CONFIG_FILE_PATH = f"COCO-InstanceSegmentation/{ARCHITECTURE}.yaml"
MAX_ITER = 3200
EVAL_PERIOD = 300
BASE_LR = 0.1
NUM_CLASSES = NUM_CLASSES


# Learning rate scheduler settings
STEPS = [1000, 1500, 2500, 4000]  # Points to decrease the learning rate
GAMMA = 0.1  # Factor to reduce the learning rate by at each step
WARMUP_ITERS = 1000  # Number of iterations to increase the learning rate from zero to BASE_LR
WARMUP_FACTOR = 0.001  # Starting factor for learning rate
WARMUP_METHOD = "linear"  # Method used for the warmup



# OUTPUT DIR
OUTPUT_DIR_PATH = os.path.join(
    TRAIN_DATA_SET_NAME,
    ARCHITECTURE,
    datetime.now().strftime('%Y-%m-%d-%H-%M-%S')
)

os.makedirs(OUTPUT_DIR_PATH, exist_ok=True)

#OUTPUT_DIR_PATH = '/content/drive/MyDrive/AMPBA/Capstone/Model/train/Train_Final/mask_rcnn_R_101_FPN_3x/2024-05-11-10-03-28'

os.environ["CUDA_LAUNCH_BLOCKING"] = "1"

cfg = get_cfg()
cfg.merge_from_file(model_zoo.get_config_file(CONFIG_FILE_PATH))
cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(CONFIG_FILE_PATH)
cfg.DATASETS.TRAIN = (TRAIN_DATA_SET_NAME,)
cfg.DATASETS.TEST = (TEST_DATA_SET_NAME,)
cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 1080
cfg.TEST.EVAL_PERIOD = EVAL_PERIOD
cfg.DATALOADER.NUM_WORKERS = 32
cfg.SOLVER.IMS_PER_BATCH = 16
cfg.INPUT.MASK_FORMAT='bitmask'
cfg.SOLVER.BASE_LR = BASE_LR
cfg.SOLVER.STEPS = STEPS
cfg.SOLVER.GAMMA = GAMMA

# Warmup
cfg.SOLVER.WARMUP_FACTOR = WARMUP_FACTOR
cfg.SOLVER.WARMUP_ITERS = WARMUP_ITERS
cfg.SOLVER.WARMUP_METHOD = WARMUP_METHOD

cfg.SOLVER.MAX_ITER = MAX_ITER
cfg.MODEL.ROI_HEADS.NUM_CLASSES = NUM_CLASSES
cfg.OUTPUT_DIR = OUTPUT_DIR_PATH
cfg.MODEL.DEVICE = torch.cuda.current_device()


# define Trainer
trainer = DefaultTrainer(cfg)
trainer.resume_or_load(resume=False)

trainer.train()

# Commented out IPython magic to ensure Python compatibility.
# Look at training curves in tensorboard:
# %load_ext tensorboard
# %tensorboard --logdir $OUTPUT_DIR_PATH

import yaml
from detectron2.config import CfgNode


# Convert the configuration to a YAML format string
cfg_yaml = cfg.dump()  # This serializes the configuration to a YAML formatted string.

# Define the path for the YAML configuration file
yaml_file_path = os.path.join(OUTPUT_DIR_PATH, "config_1080.yaml")

# Write the configuration to the file
with open(yaml_file_path, 'w') as file:
    file.write(cfg_yaml)




# Assuming `trainer` is your DefaultTrainer object and training is complete
final_weights_path = os.path.join(cfg.OUTPUT_DIR, "model_final_1080.pth")
# Save the model weights
torch.save(trainer.model.state_dict(), final_weights_path)

image = cv2.imread("../test/Test-5-_jpeg.rf.c4eea126e2fec0465791414ffc535726.jpg")
cv2_imshow(image)

cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, "model_final.pth")
cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.2
predictor = DefaultPredictor(cfg)
outputs = predictor(image)
visualizer = Visualizer(image[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=0.8)
out = visualizer.draw_instance_predictions(outputs["instances"].to("cpu"))
cv2_imshow(out.get_image()[:, :, ::-1])